# ==============================================================================
# FICHIER PRINCIPAL DE L'APPLICATION STREAMLIT : app.py (Version "PRO")
# R√¥le : Chef d'orchestre du pipeline avec une interface utilisateur stylis√©e.
# ==============================================================================

# --- Imports des biblioth√®ques ---
import streamlit as st
import os
import logging
import io
from rdflib import Graph

# --- Imports des modules du projet ---
try:
    from src.llm.llm_response_generator import get_llm_direct_response, generate_response_from_reasoning_path
    from src.ontology.ontology_retriever import retrieve_relevant_facts
    from src.llm.llm_enriched_prompt_generator import generate_enriched_prompt_response
    from src.ontology.graph_interrogator import find_reasoning_path
except ImportError as e:
    st.error(f"Erreur d'importation des modules : {e}")
    st.stop()

# --- Configuration du Logger ---
log_stream = io.StringIO()
pipeline_logger = logging.getLogger("pipeline_trace")
if not pipeline_logger.handlers:
    pipeline_logger.setLevel(logging.INFO)
    handler = logging.StreamHandler(log_stream)
    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    pipeline_logger.addHandler(handler)

# --- Configuration de la Page et Style ---
st.set_page_config(layout="wide", page_title="HPC Reasoning Pipeline", page_icon="üß†")

# CSS personnalis√© pour un look "IA PRO"
st.markdown("""
<style>
    /* Style g√©n√©ral */
    .stApp {
        background-color: #f0f2f6;
    }
    /* Style des conteneurs de r√©ponse */
    [data-testid="stVerticalBlock"] > [data-testid="stVerticalBlock"] > [data-testid="stVerticalBlock"] > [data-testid="stTickBar"] {
        background-color: #ffffff;
    }
    .st-emotion-cache-1r6slb0 {
        border: 1px solid #e6e6e6;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        transition: all 0.3s ease-in-out;
    }
    .st-emotion-cache-1r6slb0:hover {
        box-shadow: 0 8px 16px rgba(0,0,0,0.2);
        border: 1px solid #cccccc;
    }
    /* Style du bouton principal */
    .stButton > button {
        border-radius: 20px;
        border: 2px solid #ff4b4b;
        color: #ff4b4b;
        background-color: transparent;
        transition: all 0.3s ease-in-out;
    }
    .stButton > button:hover {
        border-color: #ffffff;
        color: #ffffff;
        background-color: #ff4b4b;
    }
</style>
""", unsafe_allow_html=True)

# --- Initialisation du Session State ---
if 'user_question' not in st.session_state:
    st.session_state.user_question = "Comment optimiser l'utilisation des registres vectoriels AVX-512 pour un noyau de calcul stencil 3D avec des conditions aux limites irr√©guli√®res tout en minimisant les bank conflicts sur une architecture Intel Sapphire Rapids, et quelle serait la m√©trique de performance critique √† surveiller dans VTune pour ce cas pr√©cis ?"
if 'log_output' not in st.session_state:
    st.session_state.log_output = ""
if 'responses' not in st.session_state:
    st.session_state.responses = {}

# --- Barre Lat√©rale (SIDEBAR) ---
with st.sidebar:
    st.image("https://www.via-domitia.fr/sites/default/files/styles/logo/public/atoms/logo/upvd_rvb.png?itok=yq5g8mgs", width=150)
    st.title("Configuration du Pipeline")
    st.divider()
    
    ontology_path = st.text_input("Chemin vers l'ontologie (.ttl):", "ontology/hpc.ttl")

    @st.cache_data
    def load_graph(path):
        try:
            g = Graph().parse(path, format="turtle")
            return g
        except Exception:
            return None
    
    graph = load_graph(ontology_path)
    if graph:
        st.success(f"Ontologie charg√©e ({len(graph)} faits)")
    else:
        st.error("Ontologie invalide ou introuvable.")
    
    st.divider()
    st.header("Cas d'√âtude")

    benchmark_questions = {
        "Choisissez un benchmark...": st.session_state.user_question,
        "1. Arrondis Flottants": "Comment configurer l'arrondi IEEE 754 pour minimiser l'erreur accumul√©e dans une somme de Kahan sur des donn√©es CFD, tout en respectant la norme OpenMP 5.2 ?",
        "2. Alignement H√©t√©rog√®ne": "Quel padding appliquer √† un struct {double x; int y;} pour √©viter le false-sharing entre threads OpenMP sur un AMD EPYC 9654 ?",
        "3. Communication Hybride": "Comment optimiser MPI_Send/MPI_Recv entre 1 processus MPI par socket (2x EPYC 9654) avec 8 threads OpenMP chacun, en utilisant les shared-memory windows et les hints MPI_Info ?",
        "4. Vectorisation Critique": "Quel pragma OpenMP SIMD utiliser pour vectoriser une boucle avec d√©pendance 'simd reduction(+:sum)' sur compilateur GCC 13, et comment v√©rifier l'utilisation des registres ZMM d'AVX-512 ?",
        "5. Synchronisation NUMA-Aware": "Quelle combinaison de omp_lock_t/omp_nest_lock_t choisir pour prot√©ger une hashtable distribu√©e sur 4 NUMA nodes, avec un pattern d'acc√®s 80% lecture ?"
    }
    
    def on_question_select():
        selected_title = st.session_state.benchmark_selector
        st.session_state.user_question = benchmark_questions[selected_title]
    
    st.selectbox(
        "S√©lectionnez un cas d'√©tude pour la pr√©sentation :",
        options=list(benchmark_questions.keys()),
        key="benchmark_selector",
        on_change=on_question_select,
    )
    st.divider()

# --- INTERFACE PRINCIPALE ---
st.title("üß† Pipeline d'Analyse et de Raisonnement pour le HPC")
st.markdown("Une d√©monstration de l'augmentation des LLMs par des graphes de connaissances pour des probl√®mes d'expertise.")

user_question = st.text_area("**Posez votre question experte ici :**", value=st.session_state.user_question, height=175, key="main_question_area")

if st.button("Lancer l'analyse comparative", type="primary", use_container_width=True):
    if user_question.strip() and graph:
        with st.spinner("Analyse en cours... Les experts comparent leurs approches."):
            log_stream.truncate(0); log_stream.seek(0)
            pipeline_logger.info(f"--- NOUVELLE REQU√äTE: '{user_question}' ---")
            
            # Lancement des 3 modes
            st.session_state.responses['direct'] = get_llm_direct_response(user_question)
            
            facts_mode2 = retrieve_relevant_facts(user_question, graph)
            st.session_state.responses['enriched'] = generate_enriched_prompt_response(user_question, facts_mode2)
            st.session_state.responses['enriched_facts'] = facts_mode2
            
            reasoning_report = find_reasoning_path(user_question, graph)
            st.session_state.responses['reasoning'] = generate_response_from_reasoning_path(reasoning_report)
            st.session_state.responses['reasoning_facts'] = reasoning_report
            
            st.session_state.log_output = log_stream.getvalue()
    else:
        st.warning("Veuillez poser une question et charger une ontologie valide.")

# Affichage des r√©sultats en 3 colonnes
if st.session_state.responses.get('direct'):
    st.header("Analyse Comparative des R√©ponses")
    col1, col2, col3 = st.columns(3, gap="large")

    with col1:
        with st.container(border=True):
            st.subheader("1Ô∏è‚É£ LLM Seul (Baseline)")
            st.markdown(st.session_state.responses['direct'])

    with col2:
        with st.container(border=True):
            st.subheader("2Ô∏è‚É£ LLM + Faits (Mots-cl√©s)")
            st.markdown(st.session_state.responses['enriched'])
            with st.expander("Faits extraits par cette m√©thode"):
                st.write(st.session_state.responses['enriched_facts'] or "Aucun fait trouv√©.")

    with col3:
        with st.container(border=True):
            st.subheader("üß† Notre Expert Augment√©")
            st.markdown(st.session_state.responses['reasoning'])
            with st.expander("D√©tails du raisonnement"):
                st.json(st.session_state.responses['reasoning_facts'] or {})

# Affichage des logs
if st.session_state.log_output:
    st.divider()
    with st.expander("üìã Afficher les Logs de Tra√ßabilit√©"):
        st.code(st.session_state.log_output)